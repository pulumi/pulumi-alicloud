// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as utilities from "../utilities";

/**
 * Provides a GPDB Streaming Job resource.
 *
 * Real-time data tasks.
 *
 * For information about GPDB Streaming Job and how to use it, see [What is Streaming Job](https://www.alibabacloud.com/help/en/analyticdb/analyticdb-for-postgresql/developer-reference/api-gpdb-2016-05-03-createstreamingjob).
 *
 * > **NOTE:** Available since v1.231.0.
 *
 * ## Example Usage
 *
 * Basic Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as alicloud from "@pulumi/alicloud";
 *
 * const config = new pulumi.Config();
 * const name = config.get("name") || "terraform-example";
 * const defaultTXqb15 = new alicloud.vpc.Network("defaultTXqb15", {cidrBlock: "192.168.0.0/16"});
 * const defaultaSWhbT = new alicloud.vpc.Switch("defaultaSWhbT", {
 *     vpcId: defaultTXqb15.id,
 *     zoneId: "cn-beijing-h",
 *     cidrBlock: "192.168.1.0/24",
 * });
 * const defaulth2ghc1 = new alicloud.gpdb.Instance("defaulth2ghc1", {
 *     instanceSpec: "2C8G",
 *     description: name,
 *     segNodeNum: 2,
 *     segStorageType: "cloud_essd",
 *     instanceNetworkType: "VPC",
 *     dbInstanceCategory: "Basic",
 *     paymentType: "PayAsYouGo",
 *     sslEnabled: 0,
 *     engineVersion: "6.0",
 *     zoneId: "cn-beijing-h",
 *     vswitchId: defaultaSWhbT.id,
 *     storageSize: 50,
 *     masterCu: 4,
 *     vpcId: defaultTXqb15.id,
 *     dbInstanceMode: "StorageElastic",
 *     engine: "gpdb",
 * });
 * const default2dUszY = new alicloud.gpdb.StreamingDataService("default2dUszY", {
 *     serviceName: "example",
 *     dbInstanceId: defaulth2ghc1.id,
 *     serviceDescription: "example",
 *     serviceSpec: "8",
 * });
 * const defaultcDQItu = new alicloud.gpdb.StreamingDataSource("defaultcDQItu", {
 *     dbInstanceId: defaulth2ghc1.id,
 *     dataSourceName: "example",
 *     dataSourceConfig: JSON.stringify({
 *         brokers: "alikafka-post-cn-g4t3t4eod004-1-vpc.alikafka.aliyuncs.com:9092,alikafka-post-cn-g4t3t4eod004-2-vpc.alikafka.aliyuncs.com:9092,alikafka-post-cn-g4t3t4eod004-3-vpc.alikafka.aliyuncs.com:9092",
 *         delimiter: "|",
 *         format: "delimited",
 *         topic: "ziyuan_example",
 *     }),
 *     dataSourceType: "kafka",
 *     dataSourceDescription: "example",
 *     serviceId: default2dUszY.serviceId,
 * });
 * const _default = new alicloud.gpdb.StreamingJob("default", {
 *     account: "example_001",
 *     destSchema: "public",
 *     mode: "professional",
 *     jobName: "example-kafka",
 *     jobDescription: "example-kafka",
 *     destDatabase: "adb_sampledata_tpch",
 *     dbInstanceId: defaulth2ghc1.id,
 *     destTable: "customer",
 *     dataSourceId: defaultcDQItu.dataSourceId,
 *     password: "example_001",
 *     jobConfig: `ATABASE: adb_sampledata_tpch
 * USER: example_001
 * PASSWORD: example_001
 * HOST: gp-2zean69451zsjj139-master.gpdb.rds.aliyuncs.com
 * PORT: 5432
 * KAFKA:
 *   INPUT:
 *     SOURCE:
 *       BROKERS: alikafka-post-cn-3mp3t4ekq004-1-vpc.alikafka.aliyuncs.com:9092
 *       TOPIC: ziyuan_example
 *       FALLBACK_OFFSET: LATEST
 *     KEY:
 *       COLUMNS:
 *       - NAME: c_custkey
 *         TYPE: int
 *       FORMAT: delimited
 *       DELIMITED_OPTION:
 *         DELIMITER: \\\\'|\\\\'
 *     VALUE:
 *       COLUMNS:
 *       - NAME: c_comment
 *         TYPE: varchar
 *       FORMAT: delimited
 *       DELIMITED_OPTION:
 *         DELIMITER: \\\\'|\\\\'
 *     ERROR_LIMIT: 10
 *   OUTPUT:
 *     SCHEMA: public
 *     TABLE: customer
 *     MODE: MERGE
 *     MATCH_COLUMNS:
 *     - c_custkey
 *     ORDER_COLUMNS:
 *     - c_custkey
 *     UPDATE_COLUMNS:
 *     - c_custkey
 *     MAPPING:
 *     - NAME: c_custkey
 *       EXPRESSION: c_custkey
 *   COMMIT:
 *     MAX_ROW: 1000
 *     MINIMAL_INTERVAL: 1000
 *     CONSISTENCY: ATLEAST
 *   POLL:
 *     BATCHSIZE: 1000
 *     TIMEOUT: 1000
 *   PROPERTIES:
 *     group.id: ziyuan_example_01
 * `,
 * });
 * ```
 *
 * ðŸ“š Need more examples? VIEW MORE EXAMPLES
 *
 * ## Import
 *
 * GPDB Streaming Job can be imported using the id, e.g.
 *
 * ```sh
 * $ pulumi import alicloud:gpdb/streamingJob:StreamingJob example <db_instance_id>:<job_id>
 * ```
 */
export class StreamingJob extends pulumi.CustomResource {
    /**
     * Get an existing StreamingJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: StreamingJobState, opts?: pulumi.CustomResourceOptions): StreamingJob {
        return new StreamingJob(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'alicloud:gpdb/streamingJob:StreamingJob';

    /**
     * Returns true if the given object is an instance of StreamingJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is StreamingJob {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === StreamingJob.__pulumiType;
    }

    /**
     * The name of the database account.
     */
    declare public readonly account: pulumi.Output<string | undefined>;
    /**
     * The delivery guarantee setting.
     *
     * Valid values:
     *
     * - ATLEAST
     * - EXACTLY
     */
    declare public readonly consistency: pulumi.Output<string | undefined>;
    /**
     * The creation time of the resource
     */
    declare public /*out*/ readonly createTime: pulumi.Output<string>;
    /**
     * The data source ID.
     */
    declare public readonly dataSourceId: pulumi.Output<string>;
    /**
     * The instance ID.
     */
    declare public readonly dbInstanceId: pulumi.Output<string>;
    /**
     * Target Field
     */
    declare public readonly destColumns: pulumi.Output<string[] | undefined>;
    /**
     * The name of the destination database.
     */
    declare public readonly destDatabase: pulumi.Output<string | undefined>;
    /**
     * Target Schema
     */
    declare public readonly destSchema: pulumi.Output<string | undefined>;
    /**
     * The name of the destination table.
     */
    declare public readonly destTable: pulumi.Output<string | undefined>;
    /**
     * The number of allowed error rows. Write failures occur when Kafka data does not match the destination table in AnalyticDB for PostgreSQL. If the specified value is exceeded, the job fails.
     */
    declare public readonly errorLimitCount: pulumi.Output<number | undefined>;
    /**
     * Automatic offset reset
     */
    declare public readonly fallbackOffset: pulumi.Output<string | undefined>;
    /**
     * Group Name
     */
    declare public readonly groupName: pulumi.Output<string | undefined>;
    /**
     * The YAML configuration file of the job. This parameter must be specified when Mode is set to professional.
     */
    declare public readonly jobConfig: pulumi.Output<string | undefined>;
    /**
     * The description of the job.
     */
    declare public readonly jobDescription: pulumi.Output<string | undefined>;
    /**
     * The job ID.
     */
    declare public /*out*/ readonly jobId: pulumi.Output<string>;
    /**
     * The name of the job.
     */
    declare public readonly jobName: pulumi.Output<string>;
    /**
     * Match Field
     */
    declare public readonly matchColumns: pulumi.Output<string[] | undefined>;
    /**
     * The configuration mode. Valid values:
     *
     * 1.  basic: In basic mode, you must configure the configuration parameters.
     *
     * 2.  professional: In professional mode, you can submit a YAML configuration file.
     */
    declare public readonly mode: pulumi.Output<string | undefined>;
    /**
     * The password of the database account.
     */
    declare public readonly password: pulumi.Output<string | undefined>;
    /**
     * Source Field
     */
    declare public readonly srcColumns: pulumi.Output<string[] | undefined>;
    /**
     * Service status, value:
     */
    declare public /*out*/ readonly status: pulumi.Output<string>;
    /**
     * Specifies whether to test the real-time job. Valid values:
     *
     * - true
     * - false
     *
     * Default value: false.
     */
    declare public readonly tryRun: pulumi.Output<boolean | undefined>;
    /**
     * Update Field
     */
    declare public readonly updateColumns: pulumi.Output<string[] | undefined>;
    /**
     * The write mode.
     *
     * Valid values:
     *
     * - insert
     * - update
     * - merge
     */
    declare public readonly writeMode: pulumi.Output<string | undefined>;

    /**
     * Create a StreamingJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: StreamingJobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: StreamingJobArgs | StreamingJobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as StreamingJobState | undefined;
            resourceInputs["account"] = state?.account;
            resourceInputs["consistency"] = state?.consistency;
            resourceInputs["createTime"] = state?.createTime;
            resourceInputs["dataSourceId"] = state?.dataSourceId;
            resourceInputs["dbInstanceId"] = state?.dbInstanceId;
            resourceInputs["destColumns"] = state?.destColumns;
            resourceInputs["destDatabase"] = state?.destDatabase;
            resourceInputs["destSchema"] = state?.destSchema;
            resourceInputs["destTable"] = state?.destTable;
            resourceInputs["errorLimitCount"] = state?.errorLimitCount;
            resourceInputs["fallbackOffset"] = state?.fallbackOffset;
            resourceInputs["groupName"] = state?.groupName;
            resourceInputs["jobConfig"] = state?.jobConfig;
            resourceInputs["jobDescription"] = state?.jobDescription;
            resourceInputs["jobId"] = state?.jobId;
            resourceInputs["jobName"] = state?.jobName;
            resourceInputs["matchColumns"] = state?.matchColumns;
            resourceInputs["mode"] = state?.mode;
            resourceInputs["password"] = state?.password;
            resourceInputs["srcColumns"] = state?.srcColumns;
            resourceInputs["status"] = state?.status;
            resourceInputs["tryRun"] = state?.tryRun;
            resourceInputs["updateColumns"] = state?.updateColumns;
            resourceInputs["writeMode"] = state?.writeMode;
        } else {
            const args = argsOrState as StreamingJobArgs | undefined;
            if (args?.dataSourceId === undefined && !opts.urn) {
                throw new Error("Missing required property 'dataSourceId'");
            }
            if (args?.dbInstanceId === undefined && !opts.urn) {
                throw new Error("Missing required property 'dbInstanceId'");
            }
            if (args?.jobName === undefined && !opts.urn) {
                throw new Error("Missing required property 'jobName'");
            }
            resourceInputs["account"] = args?.account;
            resourceInputs["consistency"] = args?.consistency;
            resourceInputs["dataSourceId"] = args?.dataSourceId;
            resourceInputs["dbInstanceId"] = args?.dbInstanceId;
            resourceInputs["destColumns"] = args?.destColumns;
            resourceInputs["destDatabase"] = args?.destDatabase;
            resourceInputs["destSchema"] = args?.destSchema;
            resourceInputs["destTable"] = args?.destTable;
            resourceInputs["errorLimitCount"] = args?.errorLimitCount;
            resourceInputs["fallbackOffset"] = args?.fallbackOffset;
            resourceInputs["groupName"] = args?.groupName;
            resourceInputs["jobConfig"] = args?.jobConfig;
            resourceInputs["jobDescription"] = args?.jobDescription;
            resourceInputs["jobName"] = args?.jobName;
            resourceInputs["matchColumns"] = args?.matchColumns;
            resourceInputs["mode"] = args?.mode;
            resourceInputs["password"] = args?.password;
            resourceInputs["srcColumns"] = args?.srcColumns;
            resourceInputs["tryRun"] = args?.tryRun;
            resourceInputs["updateColumns"] = args?.updateColumns;
            resourceInputs["writeMode"] = args?.writeMode;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["jobId"] = undefined /*out*/;
            resourceInputs["status"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(StreamingJob.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering StreamingJob resources.
 */
export interface StreamingJobState {
    /**
     * The name of the database account.
     */
    account?: pulumi.Input<string>;
    /**
     * The delivery guarantee setting.
     *
     * Valid values:
     *
     * - ATLEAST
     * - EXACTLY
     */
    consistency?: pulumi.Input<string>;
    /**
     * The creation time of the resource
     */
    createTime?: pulumi.Input<string>;
    /**
     * The data source ID.
     */
    dataSourceId?: pulumi.Input<string>;
    /**
     * The instance ID.
     */
    dbInstanceId?: pulumi.Input<string>;
    /**
     * Target Field
     */
    destColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The name of the destination database.
     */
    destDatabase?: pulumi.Input<string>;
    /**
     * Target Schema
     */
    destSchema?: pulumi.Input<string>;
    /**
     * The name of the destination table.
     */
    destTable?: pulumi.Input<string>;
    /**
     * The number of allowed error rows. Write failures occur when Kafka data does not match the destination table in AnalyticDB for PostgreSQL. If the specified value is exceeded, the job fails.
     */
    errorLimitCount?: pulumi.Input<number>;
    /**
     * Automatic offset reset
     */
    fallbackOffset?: pulumi.Input<string>;
    /**
     * Group Name
     */
    groupName?: pulumi.Input<string>;
    /**
     * The YAML configuration file of the job. This parameter must be specified when Mode is set to professional.
     */
    jobConfig?: pulumi.Input<string>;
    /**
     * The description of the job.
     */
    jobDescription?: pulumi.Input<string>;
    /**
     * The job ID.
     */
    jobId?: pulumi.Input<string>;
    /**
     * The name of the job.
     */
    jobName?: pulumi.Input<string>;
    /**
     * Match Field
     */
    matchColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The configuration mode. Valid values:
     *
     * 1.  basic: In basic mode, you must configure the configuration parameters.
     *
     * 2.  professional: In professional mode, you can submit a YAML configuration file.
     */
    mode?: pulumi.Input<string>;
    /**
     * The password of the database account.
     */
    password?: pulumi.Input<string>;
    /**
     * Source Field
     */
    srcColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Service status, value:
     */
    status?: pulumi.Input<string>;
    /**
     * Specifies whether to test the real-time job. Valid values:
     *
     * - true
     * - false
     *
     * Default value: false.
     */
    tryRun?: pulumi.Input<boolean>;
    /**
     * Update Field
     */
    updateColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The write mode.
     *
     * Valid values:
     *
     * - insert
     * - update
     * - merge
     */
    writeMode?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a StreamingJob resource.
 */
export interface StreamingJobArgs {
    /**
     * The name of the database account.
     */
    account?: pulumi.Input<string>;
    /**
     * The delivery guarantee setting.
     *
     * Valid values:
     *
     * - ATLEAST
     * - EXACTLY
     */
    consistency?: pulumi.Input<string>;
    /**
     * The data source ID.
     */
    dataSourceId: pulumi.Input<string>;
    /**
     * The instance ID.
     */
    dbInstanceId: pulumi.Input<string>;
    /**
     * Target Field
     */
    destColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The name of the destination database.
     */
    destDatabase?: pulumi.Input<string>;
    /**
     * Target Schema
     */
    destSchema?: pulumi.Input<string>;
    /**
     * The name of the destination table.
     */
    destTable?: pulumi.Input<string>;
    /**
     * The number of allowed error rows. Write failures occur when Kafka data does not match the destination table in AnalyticDB for PostgreSQL. If the specified value is exceeded, the job fails.
     */
    errorLimitCount?: pulumi.Input<number>;
    /**
     * Automatic offset reset
     */
    fallbackOffset?: pulumi.Input<string>;
    /**
     * Group Name
     */
    groupName?: pulumi.Input<string>;
    /**
     * The YAML configuration file of the job. This parameter must be specified when Mode is set to professional.
     */
    jobConfig?: pulumi.Input<string>;
    /**
     * The description of the job.
     */
    jobDescription?: pulumi.Input<string>;
    /**
     * The name of the job.
     */
    jobName: pulumi.Input<string>;
    /**
     * Match Field
     */
    matchColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The configuration mode. Valid values:
     *
     * 1.  basic: In basic mode, you must configure the configuration parameters.
     *
     * 2.  professional: In professional mode, you can submit a YAML configuration file.
     */
    mode?: pulumi.Input<string>;
    /**
     * The password of the database account.
     */
    password?: pulumi.Input<string>;
    /**
     * Source Field
     */
    srcColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies whether to test the real-time job. Valid values:
     *
     * - true
     * - false
     *
     * Default value: false.
     */
    tryRun?: pulumi.Input<boolean>;
    /**
     * Update Field
     */
    updateColumns?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The write mode.
     *
     * Valid values:
     *
     * - insert
     * - update
     * - merge
     */
    writeMode?: pulumi.Input<string>;
}
