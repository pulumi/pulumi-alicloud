// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.alicloud.gpdb;

import com.pulumi.alicloud.Utilities;
import com.pulumi.alicloud.gpdb.HadoopDataSourceArgs;
import com.pulumi.alicloud.gpdb.inputs.HadoopDataSourceState;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import java.lang.Integer;
import java.lang.String;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Provides a GPDB Hadoop Data Source resource.
 * 
 * Hadoop DataSource Config.
 * 
 * For information about GPDB Hadoop Data Source and how to use it, see [What is Hadoop Data Source](https://www.alibabacloud.com/help/en/analyticdb/analyticdb-for-postgresql/developer-reference/api-gpdb-2016-05-03-createhadoopdatasource).
 * 
 * &gt; **NOTE:** Available since v1.230.0.
 * 
 * ## Example Usage
 * 
 * Basic Usage
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.alicloud.AlicloudFunctions;
 * import com.pulumi.alicloud.inputs.GetZonesArgs;
 * import com.pulumi.alicloud.vpc.VpcFunctions;
 * import com.pulumi.alicloud.vpc.inputs.GetNetworksArgs;
 * import com.pulumi.alicloud.vpc.inputs.GetSwitchesArgs;
 * import com.pulumi.alicloud.ecs.EcsKeyPair;
 * import com.pulumi.alicloud.ecs.EcsKeyPairArgs;
 * import com.pulumi.alicloud.ecs.SecurityGroup;
 * import com.pulumi.alicloud.ecs.SecurityGroupArgs;
 * import com.pulumi.alicloud.ram.Role;
 * import com.pulumi.alicloud.ram.RoleArgs;
 * import com.pulumi.alicloud.resourcemanager.ResourcemanagerFunctions;
 * import com.pulumi.alicloud.resourcemanager.inputs.GetResourceGroupsArgs;
 * import com.pulumi.alicloud.kms.KmsFunctions;
 * import com.pulumi.alicloud.kms.inputs.GetKeysArgs;
 * import com.pulumi.alicloud.emrv2.Cluster;
 * import com.pulumi.alicloud.emrv2.ClusterArgs;
 * import com.pulumi.alicloud.emrv2.inputs.ClusterNodeGroupArgs;
 * import com.pulumi.alicloud.emrv2.inputs.ClusterNodeGroupSystemDiskArgs;
 * import com.pulumi.alicloud.emrv2.inputs.ClusterNodeAttributeArgs;
 * import com.pulumi.alicloud.gpdb.Instance;
 * import com.pulumi.alicloud.gpdb.InstanceArgs;
 * import com.pulumi.alicloud.gpdb.ExternalDataService;
 * import com.pulumi.alicloud.gpdb.ExternalDataServiceArgs;
 * import com.pulumi.alicloud.gpdb.HadoopDataSource;
 * import com.pulumi.alicloud.gpdb.HadoopDataSourceArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         final var config = ctx.config();
 *         final var name = config.get("name").orElse("terraform-example");
 *         final var default = AlicloudFunctions.getZones(GetZonesArgs.builder()
 *             .availableResourceCreation("VSwitch")
 *             .build());
 * 
 *         final var defaultGetNetworks = VpcFunctions.getNetworks(GetNetworksArgs.builder()
 *             .nameRegex("^default-NODELETING$")
 *             .build());
 * 
 *         final var defaultGetSwitches = VpcFunctions.getSwitches(GetSwitchesArgs.builder()
 *             .vpcId(defaultGetNetworks.ids()[0])
 *             .zoneId("cn-beijing-h")
 *             .build());
 * 
 *         var defaultEcsKeyPair = new EcsKeyPair("defaultEcsKeyPair", EcsKeyPairArgs.builder()
 *             .keyPairName(name)
 *             .build());
 * 
 *         var defaultSecurityGroup = new SecurityGroup("defaultSecurityGroup", SecurityGroupArgs.builder()
 *             .name(name)
 *             .vpcId(defaultGetNetworks.ids()[0])
 *             .build());
 * 
 *         var defaultRole = new Role("defaultRole", RoleArgs.builder()
 *             .name(name)
 *             .document("""
 *     {
 *         \"Statement\": [
 *         {
 *             \"Action\": \"sts:AssumeRole\",
 *             \"Effect\": \"Allow\",
 *             \"Principal\": {
 *             \"Service\": [
 *                 \"emr.aliyuncs.com\",
 *                 \"ecs.aliyuncs.com\"
 *             ]
 *             }
 *         }
 *         ],
 *         \"Version\": \"1\"
 *     }
 *             """)
 *             .description("this is a role example.")
 *             .force(true)
 *             .build());
 * 
 *         final var defaultGetResourceGroups = ResourcemanagerFunctions.getResourceGroups(GetResourceGroupsArgs.builder()
 *             .status("OK")
 *             .build());
 * 
 *         final var defaultGetKeys = KmsFunctions.getKeys(GetKeysArgs.builder()
 *             .status("Enabled")
 *             .build());
 * 
 *         var defaultCluster = new Cluster("defaultCluster", ClusterArgs.builder()
 *             .nodeGroups(            
 *                 ClusterNodeGroupArgs.builder()
 *                     .vswitchIds(defaultGetSwitches.ids()[0])
 *                     .instanceTypes("ecs.g6.xlarge")
 *                     .nodeCount(1)
 *                     .spotInstanceRemedy(false)
 *                     .dataDisks(ClusterNodeGroupDataDiskArgs.builder()
 *                         .count(3)
 *                         .category("cloud_essd")
 *                         .size(80)
 *                         .performanceLevel("PL0")
 *                         .build())
 *                     .nodeGroupName("emr-master")
 *                     .paymentType("PayAsYouGo")
 *                     .withPublicIp(false)
 *                     .gracefulShutdown(false)
 *                     .systemDisk(ClusterNodeGroupSystemDiskArgs.builder()
 *                         .category("cloud_essd")
 *                         .size(80)
 *                         .performanceLevel("PL0")
 *                         .count(1)
 *                         .build())
 *                     .nodeGroupType("MASTER")
 *                     .build(),
 *                 ClusterNodeGroupArgs.builder()
 *                     .spotInstanceRemedy(false)
 *                     .nodeGroupType("CORE")
 *                     .vswitchIds(defaultGetSwitches.ids()[0])
 *                     .nodeCount(2)
 *                     .gracefulShutdown(false)
 *                     .systemDisk(ClusterNodeGroupSystemDiskArgs.builder()
 *                         .performanceLevel("PL0")
 *                         .count(1)
 *                         .category("cloud_essd")
 *                         .size(80)
 *                         .build())
 *                     .dataDisks(ClusterNodeGroupDataDiskArgs.builder()
 *                         .count(3)
 *                         .performanceLevel("PL0")
 *                         .category("cloud_essd")
 *                         .size(80)
 *                         .build())
 *                     .nodeGroupName("emr-core")
 *                     .paymentType("PayAsYouGo")
 *                     .instanceTypes("ecs.g6.xlarge")
 *                     .withPublicIp(false)
 *                     .build())
 *             .deployMode("NORMAL")
 *             .tags(Map.ofEntries(
 *                 Map.entry("Created", "TF"),
 *                 Map.entry("For", "example")
 *             ))
 *             .releaseVersion("EMR-5.10.0")
 *             .applications(            
 *                 "HADOOP-COMMON",
 *                 "HDFS",
 *                 "YARN")
 *             .nodeAttributes(ClusterNodeAttributeArgs.builder()
 *                 .zoneId("cn-beijing-h")
 *                 .keyPairName(defaultEcsKeyPair.id())
 *                 .dataDiskEncrypted(true)
 *                 .dataDiskKmsKeyId(defaultGetKeys.ids()[0])
 *                 .vpcId(defaultGetNetworks.ids()[0])
 *                 .ramRole(defaultRole.name())
 *                 .securityGroupId(defaultSecurityGroup.id())
 *                 .build())
 *             .resourceGroupId(defaultGetResourceGroups.ids()[0])
 *             .clusterName(name)
 *             .paymentType("PayAsYouGo")
 *             .clusterType("DATAFLOW")
 *             .build());
 * 
 *         var defaultZoepvx = new Instance("defaultZoepvx", InstanceArgs.builder()
 *             .instanceSpec("2C8G")
 *             .description(name)
 *             .segNodeNum(2)
 *             .segStorageType("cloud_essd")
 *             .instanceNetworkType("VPC")
 *             .paymentType("PayAsYouGo")
 *             .sslEnabled(0)
 *             .engineVersion("6.0")
 *             .zoneId("cn-beijing-h")
 *             .vswitchId(defaultGetSwitches.ids()[0])
 *             .storageSize(50)
 *             .masterCu(4)
 *             .vpcId(defaultGetNetworks.ids()[0])
 *             .dbInstanceMode("StorageElastic")
 *             .engine("gpdb")
 *             .dbInstanceCategory("Basic")
 *             .build());
 * 
 *         var defaultyOxz1K = new ExternalDataService("defaultyOxz1K", ExternalDataServiceArgs.builder()
 *             .serviceName(name)
 *             .dbInstanceId(defaultZoepvx.id())
 *             .serviceDescription(name)
 *             .serviceSpec("8")
 *             .build());
 * 
 *         var defaultHadoopDataSource = new HadoopDataSource("defaultHadoopDataSource", HadoopDataSourceArgs.builder()
 *             .hdfsConf("aaa")
 *             .dataSourceName(defaultyOxz1K.serviceName())
 *             .yarnConf("aaa")
 *             .hiveConf("aaa")
 *             .hadoopCreateType("emr")
 *             .dataSourceDescription(name)
 *             .mapReduceConf("aaa")
 *             .dataSourceType("hive")
 *             .hadoopCoreConf("aaa")
 *             .emrInstanceId(defaultCluster.id())
 *             .dbInstanceId(defaultZoepvx.id())
 *             .hadoopHostsAddress("aaa")
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ## Import
 * 
 * GPDB Hadoop Data Source can be imported using the id, e.g.
 * 
 * ```sh
 * $ pulumi import alicloud:gpdb/hadoopDataSource:HadoopDataSource example &lt;db_instance_id&gt;:&lt;data_source_id&gt;
 * ```
 * 
 */
@ResourceType(type="alicloud:gpdb/hadoopDataSource:HadoopDataSource")
public class HadoopDataSource extends com.pulumi.resources.CustomResource {
    /**
     * Creation time
     * 
     */
    @Export(name="createTime", refs={String.class}, tree="[0]")
    private Output<String> createTime;

    /**
     * @return Creation time
     * 
     */
    public Output<String> createTime() {
        return this.createTime;
    }
    /**
     * Data Source Description
     * 
     */
    @Export(name="dataSourceDescription", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> dataSourceDescription;

    /**
     * @return Data Source Description
     * 
     */
    public Output<Optional<String>> dataSourceDescription() {
        return Codegen.optional(this.dataSourceDescription);
    }
    /**
     * The data source ID.
     * 
     */
    @Export(name="dataSourceId", refs={Integer.class}, tree="[0]")
    private Output<Integer> dataSourceId;

    /**
     * @return The data source ID.
     * 
     */
    public Output<Integer> dataSourceId() {
        return this.dataSourceId;
    }
    /**
     * Data Source Name
     * 
     */
    @Export(name="dataSourceName", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> dataSourceName;

    /**
     * @return Data Source Name
     * 
     */
    public Output<Optional<String>> dataSourceName() {
        return Codegen.optional(this.dataSourceName);
    }
    /**
     * The type of the data source. Valid values:
     * 
     * *   mysql
     * - postgresql
     * 
     * *   hdfs
     * - hive
     * 
     */
    @Export(name="dataSourceType", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> dataSourceType;

    /**
     * @return The type of the data source. Valid values:
     * 
     * *   mysql
     * - postgresql
     * 
     * *   hdfs
     * - hive
     * 
     */
    public Output<Optional<String>> dataSourceType() {
        return Codegen.optional(this.dataSourceType);
    }
    /**
     * The instance ID.
     * 
     */
    @Export(name="dbInstanceId", refs={String.class}, tree="[0]")
    private Output<String> dbInstanceId;

    /**
     * @return The instance ID.
     * 
     */
    public Output<String> dbInstanceId() {
        return this.dbInstanceId;
    }
    /**
     * The ID of the Emr instance.
     * 
     */
    @Export(name="emrInstanceId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> emrInstanceId;

    /**
     * @return The ID of the Emr instance.
     * 
     */
    public Output<Optional<String>> emrInstanceId() {
        return Codegen.optional(this.emrInstanceId);
    }
    /**
     * The string that specifies the content of the Hadoop core-site.xml file.
     * 
     */
    @Export(name="hadoopCoreConf", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> hadoopCoreConf;

    /**
     * @return The string that specifies the content of the Hadoop core-site.xml file.
     * 
     */
    public Output<Optional<String>> hadoopCoreConf() {
        return Codegen.optional(this.hadoopCoreConf);
    }
    /**
     * The type of the external service. Valid values:
     * - emr: E-MapReduce (EMR) Hadoop cluster.
     * - selfCreate: self-managed Hadoop cluster.
     * 
     */
    @Export(name="hadoopCreateType", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> hadoopCreateType;

    /**
     * @return The type of the external service. Valid values:
     * - emr: E-MapReduce (EMR) Hadoop cluster.
     * - selfCreate: self-managed Hadoop cluster.
     * 
     */
    public Output<Optional<String>> hadoopCreateType() {
        return Codegen.optional(this.hadoopCreateType);
    }
    /**
     * The IP address and hostname of the Hadoop cluster (data source) in the /etc/hosts file.
     * 
     */
    @Export(name="hadoopHostsAddress", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> hadoopHostsAddress;

    /**
     * @return The IP address and hostname of the Hadoop cluster (data source) in the /etc/hosts file.
     * 
     */
    public Output<Optional<String>> hadoopHostsAddress() {
        return Codegen.optional(this.hadoopHostsAddress);
    }
    /**
     * The string that specifies the content of the Hadoop hdfs-site.xml file. This parameter must be specified when DataSourceType is set to HDFS.
     * 
     */
    @Export(name="hdfsConf", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> hdfsConf;

    /**
     * @return The string that specifies the content of the Hadoop hdfs-site.xml file. This parameter must be specified when DataSourceType is set to HDFS.
     * 
     */
    public Output<Optional<String>> hdfsConf() {
        return Codegen.optional(this.hdfsConf);
    }
    /**
     * The string that specifies the content of the Hadoop hive-site.xml file. This parameter must be specified when DataSourceType is set to Hive.
     * 
     */
    @Export(name="hiveConf", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> hiveConf;

    /**
     * @return The string that specifies the content of the Hadoop hive-site.xml file. This parameter must be specified when DataSourceType is set to Hive.
     * 
     */
    public Output<Optional<String>> hiveConf() {
        return Codegen.optional(this.hiveConf);
    }
    /**
     * The content of the Hadoop mapred-site.xml file. This parameter must be specified when DataSourceType is set to HDFS.
     * 
     */
    @Export(name="mapReduceConf", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> mapReduceConf;

    /**
     * @return The content of the Hadoop mapred-site.xml file. This parameter must be specified when DataSourceType is set to HDFS.
     * 
     */
    public Output<Optional<String>> mapReduceConf() {
        return Codegen.optional(this.mapReduceConf);
    }
    /**
     * Data Source Status
     * 
     */
    @Export(name="status", refs={String.class}, tree="[0]")
    private Output<String> status;

    /**
     * @return Data Source Status
     * 
     */
    public Output<String> status() {
        return this.status;
    }
    /**
     * The string that specifies the content of the Hadoop yarn-site.xml file. This parameter must be specified when DataSourceType is set to HDFS.
     * 
     */
    @Export(name="yarnConf", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> yarnConf;

    /**
     * @return The string that specifies the content of the Hadoop yarn-site.xml file. This parameter must be specified when DataSourceType is set to HDFS.
     * 
     */
    public Output<Optional<String>> yarnConf() {
        return Codegen.optional(this.yarnConf);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public HadoopDataSource(java.lang.String name) {
        this(name, HadoopDataSourceArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public HadoopDataSource(java.lang.String name, HadoopDataSourceArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public HadoopDataSource(java.lang.String name, HadoopDataSourceArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("alicloud:gpdb/hadoopDataSource:HadoopDataSource", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private HadoopDataSource(java.lang.String name, Output<java.lang.String> id, @Nullable HadoopDataSourceState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("alicloud:gpdb/hadoopDataSource:HadoopDataSource", name, state, makeResourceOptions(options, id), false);
    }

    private static HadoopDataSourceArgs makeArgs(HadoopDataSourceArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? HadoopDataSourceArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static HadoopDataSource get(java.lang.String name, Output<java.lang.String> id, @Nullable HadoopDataSourceState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new HadoopDataSource(name, id, state, options);
    }
}
